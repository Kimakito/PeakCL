#!/usr/bin/env python3
"""
Script de v√©rification des liens internes cass√©s
Analyse tous les fichiers HTML g√©n√©r√©s par Jekyll (_site) et d√©tecte les liens internes qui pointent vers des pages inexistantes
"""
import os
import re
from pathlib import Path
try:
    from bs4 import BeautifulSoup
except ImportError:
    print("‚ö†Ô∏è  BeautifulSoup4 n'est pas install√©. Installation...")
    os.system("pip3 install beautifulsoup4")
    from bs4 import BeautifulSoup

# Dossier √† analyser (site g√©n√©r√© par Jekyll)
BASE_DIR = Path(__file__).parent.parent / '_site'

if not BASE_DIR.exists():
    print("‚ùå Le dossier _site n'existe pas. Veuillez d'abord g√©n√©rer le site avec 'bundle exec jekyll build'")
    exit(1)

# Extensions de fichiers √† analyser
HTML_EXT = ['.html']

print("üîç Analyse des liens internes (site g√©n√©r√©)...\n")

# Collecte toutes les urls internes trouv√©es
internal_links = {}  # {url: [fichiers qui la r√©f√©rencent]}
# Collecte toutes les urls de fichiers existants
existing_files = set()

# Parcourir tous les fichiers HTML
for root, dirs, files in os.walk(BASE_DIR):
    # Ignorer certains dossiers
    dirs[:] = [d for d in dirs if d not in ['node_modules', '_site', '.git', 'assets']]
    
    for file in files:
        if any(file.endswith(ext) for ext in HTML_EXT):
            filepath = Path(root) / file
            rel_path = filepath.relative_to(BASE_DIR)
            existing_files.add(str(rel_path))
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f, 'html.parser')
                    for a in soup.find_all('a', href=True):
                        href = a['href']
                        # Lien interne uniquement
                        if href.startswith('/') and not href.startswith('//'):
                            # Nettoyer l'URL (enlever le / initial et les ancres)
                            clean_url = href.lstrip('/').split('#')[0]
                            if clean_url:  # Ignorer les liens vides ou juste des ancres
                                if clean_url not in internal_links:
                                    internal_links[clean_url] = []
                                internal_links[clean_url].append(str(rel_path))
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors de la lecture de {rel_path}: {e}")

# V√©rifier les liens cass√©s
broken_links = {}
for link, sources in internal_links.items():
    # V√©rifier si le fichier existe
    target_exists = False
    for existing in existing_files:
        if existing == link or existing.endswith('/' + link):
            target_exists = True
            break
    
    if not target_exists:
        broken_links[link] = sources

# Afficher les r√©sultats
if broken_links:
    print("‚ùå Liens internes cass√©s d√©tect√©s:\n")
    for link, sources in sorted(broken_links.items()):
        print(f"  /{link}")
        print(f"    R√©f√©renc√© dans:")
        for source in set(sources):
            print(f"      - {source}")
        print()
    print(f"üìä Total: {len(broken_links)} lien(s) cass√©(s)\n")
else:
    print("‚úÖ Aucun lien interne cass√© d√©tect√©.\n")

print("‚úÖ Analyse termin√©e!")
